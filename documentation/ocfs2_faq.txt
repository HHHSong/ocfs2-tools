/* -*- mode: txt; c-basic-offset: 4; -*-
 * vim: noexpandtab sw=4 ts=4 sts=0:
 */

General
-------

Q01	How do I get started?
A01	a) Download and install the module and tools rpms.
	b) Create cluster.conf and propagate to all nodes.
	c) Configure and start the O2CB cluster service.
	d) Format the volume.
	e) Mount the volume.
==============================================================================

Download and Install
--------------------

Q01	How do I download the rpms?
A01	If you are on Novell's SLES9, upgrade to SP2 and you will have the
	required module installed. However, you will be required to install
	ocfs2-tools and ocfs2console rpms from the distribution.
	If you are on Red Hat's EL4, download and install the appropriate module
	rpm and the two tools rpms, ocfs2-tools and ocfs2console. Appropriate
	module refers to one matching the kernel flavor, uniprocessor, smp or
	hugemem.

Q02	How do I install the rpms?
A02	You can install all three rpms in one go using:
	rpm -ivh ocfs2-tools-X.i386.rpm ocfs2console-X.i386.rpm
		ocfs2-2.6.9-11.ELsmp-X.i686.rpm
	If you need to upgrade, do:
	rpm -Uvh ocfs2-2.6.9-11.ELsmp-Y.i686.rpm

Q03	Do I need to install the console?
A03	No, the console is recommended but not required.

Q04	What are the dependencies for installing ocfs2console?
A04	ocfs2console requires e2fsprogs, glib2 2.2.3 or later, vte 0.11.10 or
	later, pygtk2 (EL4) or python-gtk (SLES9) 1.99.16 or later,
	python 1.99.16 or later and ocfs2-tools.

Q05	What modules are installed with the OCFS2 package?
A05	a) configfs.ko
	b) ocfs2.ko
	c) ocfs2_dlm.ko
	d) ocfs2_dlmfs.ko
	e) ocfs2_nodemanager.ko

Q06	What tools are installed with the tools package?
A06	a) mkfs.ocfs2
	b) fsck.ocfs2
	c) tunefs.ocfs2
	d) debugfs.ocfs2
	e) mount.ocfs2
	f) mounted.ocfs2
	g) ocfs2cdsl
	h) ocfs2_hb_ctl
	i) o2cb_ctl
	j) ocfs2console - installed with the console package
==============================================================================

Configure
---------

Q01	How do I populate /etc/ocfs2/cluster.conf?
A01	If you have installed the console, use it to create this
	configuration file. For details, refer to the user's guide.
	If you do not have the console installed, check the Appendix in the
	User's guide for a sample cluster.conf and the details of all the
	components. 
	Do not forget to copy this file to all the nodes in the cluster.
	If you ever edit this file on any node, ensure the other nodes are
	updated as well.
==============================================================================

O2CB Cluster Service
--------------------

Q01	How do I configure the cluster service?
A01	# /etc/init.d/o2cb configure
	Enter 'y' if you want the service to load on boot and the name of
	the cluster (as listed in /etc/ocfs2/cluster.conf).

Q02	How do I start the cluster service?
A02	a) To load the modules, do:
		# /etc/init.d/o2cb load
	b) To Online it, do:
		# /etc/init.d/o2cb online [cluster_name]
	If you have configured the cluster to load on boot, you could
	combine the two as follows:
		# /etc/init.d/o2cb start [cluster_name]
	The cluster name is not required if you have specified the name
	during configuration.

Q03	How do I stop the cluster service?
A03	a) To offline it, do:
		# /etc/init.d/o2cb offline [cluster_name]
	b) To unload the modules, do:
		# /etc/init.d/o2cb unload
	If you have configured the cluster to load on boot, you could
	combine the two as follows:
		# /etc/init.d/o2cb stop [cluster_name]
	The cluster name is not required if you have specified the name
	during configuration.

Q04	How can I learn the status of the cluster?
A04	To learn the status of the cluster, do:
		# /etc/init.d/o2cb status

Q05	I am unable to get the cluster online. What could be wrong?
A05	Check whether the node name in the cluster.conf exactly matches the
	hostname. One of the nodes in the cluster.conf need to be in the
	cluster for the cluster to be online.
==============================================================================

Format
------

Q01	How do I format a volume?
A01	You could either use the console or use mkfs.ocfs2 directly to format
	the volume.  For console, refer to the user's guide.
		# mkfs.ocfs2 -L "oracle_home" /dev/sdX
	The above formats the volume with default block and cluster sizes,
	which are computed based upon the size of the volume.
		# mkfs.ocfs2 -b 4k -C 32K -L "oracle_home" -N 4 /dev/sdX
	The above formats the volume for 4 nodes with a 4K block size and a
	32K cluster size.

Q02	What does the number of node slots during format refer to?
A02	The number of node slots specifies the number of nodes that can
	concurrently mount the volume. This number is specified during
	format and can be increased using tunefs.ocfs2. This number cannot
	be decreased.

Q03	What should I consider when determining the number of node slots?
A03	OCFS2 allocates system files, like Journal, for each node slot.
	So as to not to waste space, one should specify a number within the
	ballpark of the actual number of nodes. Also, as this number can be
	increased, there is no need to specify a much larger number than one
	plans for mounting the volume.

Q04	Does the number of node slots have to be the same for all volumes?
A04	No. This number can be different for each volume.

Q05	What block size should I use?
A05	A block size is the smallest unit of space addressable by the file
	system. OCFS2 supports block sizes of 512 bytes, 1K, 2K and 4K.
	The block size cannot be changed after the format. For most volume
	sizes, a 4K size is recommended. On the other hand, the 512 bytes
	block is never recommended.

Q06	What cluster size should I use?
A06	A cluster size is the smallest unit of space allocated to a file to
	hold the data. OCFS2 supports cluster sizes of 4K, 8K, 16K, 32K,
	64K, 128K, 256K, 512K and 1M. For database volumes, a cluster size
	of 128K or larger is recommended. For Oracle home, 32K to 64K.

Q07	Any advantage of labelling the volumes?
A07	As in a shared disk environment, the disk name (/dev/sdX) for a
	particular device be different on different nodes, labelling becomes
	a must for easy identification.
	You could also use labels to identify volumes during mount.
		# mount -L "label" /dir
	The volume label is changeable using the tunefs.ocfs2 utility.
==============================================================================

Mount
-----

Q01	How do I mount the volume?
A01	You could either use the console or use mount directly. For console,
	refer to the user's guide.
		# mount -t ocfs2 /dev/sdX /dir
	The above command will mount device /dev/sdX on directory /dir.

Q02	How do I mount by label?
A02	To mount by label do:
		# mount -L "label" /dir

Q03	What entry to I add to /etc/fstab to mount an ocfs2 volume?
A03	Add the following:
		/dev/sdX	/dir	ocfs2	noauto,_netdev	0	0
	The _netdev option indicates that the devices needs to be mounted after
	the network is up.

Q04	What all do I need to do to automount OCFS2 volumes on boot?
A04	a) Enable o2cb service using:
		# chkconfig --add o2cb
	b) Configure o2cb to load on boot using:
		# /etc/init.d/o2cb configure
	c) Add entries into /etc/fstab as follows:
		/dev/sdX	/dir	ocfs2	_netdev	0	0

Q05	How do I know my volume is mounted?
A05	a) Enter mount without arguments, or
		# mount
	b) List /etc/mtab, or
		# cat /etc/mtab
	c) List /proc/mounts
		# cat /proc/mounts
	mount command reads the /etc/mtab to show the information.

Q06	What are the /config and /dlm mountpoints for?
A06	OCFS2 comes bundled with two in-memory filesystems configfs and
	ocfs2_dlmfs. configfs is used by the ocfs2 tools to communicate to the
	in-kernel node manager the list of nodes in the cluster and to the
	in-kernel heartbeat thread the resource to heartbeat on.
	ocfs2_dlmfs is used by ocfs2 tools to communicate with the in-kernel
	dlm to take and release clusterwide locks on resources.
==============================================================================

Oracle RAC
----------

Q01	Any special flags to run Oracle RAC?
A01	OCFS2 volumes containing the Voting diskfile (CRS), Cluster registry
	(OCR), Data files, Redo logs, Archive logs and control files should
	be mounted with the "datavolume" mount option. This is to ensure
	that the Oracle processes open these files with the o_direct flag.

Q02	What about the volume containing Oracle home?
A02	Oracle home volume should be mounted normally, that is, without the
	"datavolume" mount option. This mount option is only relevant for
	Oracle files listed above.

Q03	Does that mean I cannot have my data file and Oracle home on the
	same volume?
A03	Yes. The volume containing the Oracle data files, redo-logs, etc.
	should never be on the same volume as the distribution (including the
	trace logs like, alert.log).
==============================================================================

Moving data from OCFS (Release 1) and OCFS2
-------------------------------------------

Q01	Can I mount OCFS volumes as OCFS2?
A01	No. OCFS and OCFS2 are not on-disk compatible. We had to break the
	compatibility in order to add many of the new features. At the same
	time, we have added enough flexibility in the new disk layout so as to
	maintain backward compatibility in the future.

Q02	Can OCFS volumes and OCFS2 volumes be mounted on the same machine
	simultaneously?
A02	No. OCFS only works on 2.4 linux kernels (Red Hat's AS2.1/EL3 and SuSE's
	SLES8).  OCFS2, on the other hand, only works on the 2.6 kernels
	(Red Hat's EL4 and SuSE's SLES9).

Q03	Can I access my OCFS volume on 2.6 kernels (SLES9/RHEL4)?
A03	Yes, you can access the OCFS volume on 2.6 kernels using FSCat
	tools, fsls and fscp. These tools can access the OCFS volumes at the
	device layer, to list and copy the files to another filesystem.
	FSCat tools are available on oss.oracle.com.

Q04	Can I in-place convert my OCFS volume to OCFS2?
A04	No. The on-disk layout of OCFS and OCFS2 are sufficiently different
	that it would require a third disk (as a temporary buffer) inorder to
	in-place upgrade the volume. With that in mind, it was decided not to
	develop such a tool but instead provide tools to copy data from OCFS
	without one having to mount it.

Q05	What is the quickest way to move data from OCFS to OCFS2?
A05	Quickest would mean having to perform the minimal number of copies.
	If you have the current backup on a non-OCFS volume accessible from
	the 2.6 kernel install, then all you would need to do is to retore
	the backup on the OCFS2 volume(s). If you do not have a backup but
	have a setup in which the system containing the OCFS2 volumes can
	access the disks containing the OCFS volume, you can use the FSCat
	tools to extract data from the OCFS volume and copy onto OCFS2.
==============================================================================

Coreutils
---------

Q01	Like with OCFS (Release 1), do I need to use o_direct enabled tools
	to perform cp, mv, tar, etc.?
A01	No. OCFS2 does not need the o_direct enabled tools. The file system
	allows processes to open files in both o_direct and bufferred mode
	concurrently.
==============================================================================

PROC interface
--------------

Q01	How do I know what version number of the OCFS2?
A01	# cat /proc/fs/ocfs2/version
	OCFS2 1.0.0 Tue Aug  2 17:38:59 PDT 2005 (build e7bd36709a2c1cb875cf2d533a018f20)

root@ca-test32:~# mounted.ocfs2 -d
Device                FS     UUID                                  Label
/dev/sdb1             ocfs2  e70c75a0-a08c-480a-bf50-ebda4191da30 mm_v2_dbf1
/dev/sdb2             ocfs2  f49163e8-6288-43c4-a792-e9401fde45fa mm_v2_ctrl
/dev/sdb3             ocfs2  2d441be2-adb6-4c52-9e19-9a7c2c485dc4 mm_v2_ocfs2
/dev/sdb5             ocfs2  8607eae9-8e4f-4495-84e8-8db0bc9da60c mm_v2_log1
/dev/sdb6             ocfs2  acfb7b7d-a277-4741-9791-620ea9b82670 mm_v2_log2
/dev/sdf1             ocfs2  84749537-df45-4a97-aa28-fad63760b674 ocfs2test
/dev/sdi2             ocfs2  5c30cc8d-ee31-414c-bc6c-a8685d738262
/dev/sdq1             ocfs2  dca2901b-241f-406e-80e9-85dd6b452d1a oracle_home
/dev/sdcf1            ocfs2  663764bd-1eed-4b3c-aa48-a98f0be0e574 ocfs2test
/dev/sdcf2            ocfs2  8e2d9c21-ef47-4fea-8ac5-2306cc60455e mm_v2_log2
/dev/sdcg1            ocfs2  663764bd-1eed-4b3c-aa48-a98f0be0e574 ocfs2test


root@ca-test32:~# mounted.ocfs2 -f
Device                FS     Nodes
/dev/sdb1             ocfs2  ca-test91
/dev/sdb2             ocfs2  ca-test91
/dev/sdb3             ocfs2  ca-test91
/dev/sdb5             ocfs2  ca-test91
/dev/sdb6             ocfs2  ca-test91, ca-test90
/dev/sdf1             ocfs2  Not mounted
/dev/sdi2             ocfs2  ca-test34, ca-test35
/dev/sdq1             ocfs2  Not mounted
/dev/sdcf1            ocfs2  Not mounted
/dev/sdcf2            ocfs2  Not mounted
/dev/sdcg1            ocfs2  Not mounted
/dev/sdcg2            ocfs2  Not mounted
/dev/sdch1            ocfs2  ca-test30
/dev/sdci1            ocfs2  ca-test30
/dev/sdcj1            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30
/dev/sdcj2            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30
/dev/sdck1            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30
/dev/sdck2            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30
/dev/sdcl1            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30
/dev/sdcl2            ocfs2  ca-test31, ca-test33, ca-test32, ca-test30




root@ca-test32:/ocfs2/archives# ocfs2cdsl  agent.ora
root@ca-test32:/ocfs2/archives# ll
total 8
drwxr-xr-x  2 root root 4096 Aug  8 11:41 .
drwxr-xr-x  5 root root 4096 Aug  8 11:41 ..
lrwxrwxrwx  1 root root   50 Aug  8 11:41 agent.ora -> ../.cluster/hostname/{hostname}/archives/agent.ora






root@ca-test32:/ocfs2/archives# cp /root/.bash_profile bash_profile
root@ca-test32:/ocfs2/archives# ll
total 9
drwxr-xr-x  2 root root 4096 Aug  8 11:50 .
drwxr-xr-x  5 root root 4096 Aug  8 11:41 ..
lrwxrwxrwx  1 root root   50 Aug  8 11:41 agent.ora -> ../.cluster/hostname/{hostname}/archives/agent.ora
-rw-r--r--  1 root root  191 Aug  8 11:50 bash_profile
root@ca-test32:/ocfs2/archives# ocfs2cdsl -c bash_profile
root@ca-test32:/ocfs2/archives# ll
total 8
drwxr-xr-x  2 root root 4096 Aug  8 11:50 .
drwxr-xr-x  5 root root 4096 Aug  8 11:41 ..
lrwxrwxrwx  1 root root   50 Aug  8 11:41 agent.ora -> ../.cluster/hostname/{hostname}/archives/agent.ora
lrwxrwxrwx  1 root root   53 Aug  8 11:50 bash_profile -> ../.cluster/hostname/{hostname}/archives/bash_profile


